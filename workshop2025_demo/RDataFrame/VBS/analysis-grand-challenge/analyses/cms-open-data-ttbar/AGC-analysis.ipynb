{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd53d8d1-a8e9-452a-8463-6a44a0525e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "from types import SimpleNamespace\n",
    "\n",
    "#redirector='file:///shared-scratch/cms/store/user/AGC'\n",
    "redirector='file:///scratch/cms/store/user/AGC/'\n",
    "#redirector='file:///shared/cms/store/user/AGC/'\n",
    "sched_port=20012\n",
    "\n",
    "# Mimic command-line arguments using SimpleNamespace\n",
    "args = SimpleNamespace(\n",
    "    n_max_files_per_sample=None,  # None to use all files\n",
    "    data_cache=None,  # or path to local data\n",
    "    remote_data_prefix=redirector,\n",
    "    output=\"agc-out.root\",\n",
    "    inference=False,\n",
    "    scheduler=\"dask-remote\",  # choices=[\"mt\", \"dask-local\", \"dask-ssh\", \"dask-remote\"]\n",
    "    scheduler_address=\"tcp://127.0.0.1:\"+str(sched_port),\n",
    "    ncores=96, # multiprocessing.cpu_count()\n",
    "    npartitions=92*3,\n",
    "    hosts=None,\n",
    "    verbose=True,\n",
    "    statistical_validation=False,\n",
    "    no_fitting=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b26ea6-c0d0-4abd-bbff-cbe83032b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "from typing import Tuple\n",
    "\n",
    "import ml\n",
    "import ROOT\n",
    "from distributed import Client, LocalCluster, SSHCluster, get_worker\n",
    "from plotting import save_ml_plots, save_plots\n",
    "from statistical import fit_histograms\n",
    "from utils import AGCInput, AGCResult, postprocess_results, retrieve_inputs, save_histos\n",
    "\n",
    "# Using https://atlas-groupdata.web.cern.ch/atlas-groupdata/dev/AnalysisTop/TopDataPreparation/XSection-MC15-13TeV.data\n",
    "# as a reference. Values are in pb.\n",
    "XSEC_INFO = {\n",
    "    \"ttbar\": 396.87 + 332.97,  # nonallhad + allhad, keep same x-sec for all\n",
    "    \"single_top_s_chan\": 2.0268 + 1.2676,\n",
    "    \"single_top_t_chan\": (36.993 + 22.175) / 0.252,  # scale from lepton filter to inclusive\n",
    "    \"single_top_tW\": 37.936 + 37.906,\n",
    "    \"wjets\": 61457 * 0.252,  # e/mu+nu final states\n",
    "}\n",
    "\n",
    "def transfer_to_tier(dask_worker):\n",
    "    import os\n",
    "    os.popen('xrdcp --force \"./agc-out.root\" root://eosuser.cern.ch//eos/user/l/lpaciose/agc-out.root;')\n",
    "\n",
    "    return True\n",
    "\n",
    "def create_dask_client(scheduler: str, ncores: int, hosts: str, scheduler_address: str) -> Client:\n",
    "    \"\"\"Create a Dask distributed client.\"\"\"\n",
    "    if scheduler == \"dask-local\":\n",
    "        lc = LocalCluster(n_workers=ncores, threads_per_worker=1, processes=True)\n",
    "        return Client(lc)\n",
    "\n",
    "    if scheduler == \"dask-ssh\":\n",
    "        workers = hosts.split(\",\")\n",
    "        print(f\"Using worker nodes: {workers=}\")\n",
    "        # The creation of the SSHCluster object might need to be further configured to fit specific use cases.\n",
    "        # For example, in some clusters the \"local_directory\" key must be supplied in the worker_options dictionary.\n",
    "        sshc = SSHCluster(\n",
    "            workers,\n",
    "            connect_options={\"known_hosts\": None},\n",
    "            worker_options={\n",
    "                \"nprocs\": ncores,\n",
    "                \"nthreads\": 1,\n",
    "                \"memory_limit\": \"32GB\",\n",
    "            },\n",
    "        )\n",
    "        return Client(sshc)\n",
    "\n",
    "    if scheduler == \"dask-remote\":\n",
    "        return Client(scheduler_address)\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"Unexpected scheduling mode '{scheduler}'. Valid modes are ['dask-local', 'dask-ssh', 'dask-remote'].\"\n",
    "    )\n",
    "\n",
    "\n",
    "def define_trijet_mass(df: ROOT.RDataFrame) -> ROOT.RDataFrame:\n",
    "    \"\"\"Add the trijet_mass observable to the dataframe after applying the appropriate selections.\"\"\"\n",
    "\n",
    "    # First, select events with at least 2 b-tagged jets\n",
    "    df = df.Filter(\"Sum(Jet_btagCSVV2_cut > 0.5) > 1\")\n",
    "\n",
    "    # Build four-momentum vectors for each jet\n",
    "    df = df.Define(\n",
    "        \"Jet_p4\",\n",
    "        \"ConstructP4(Jet_pt_cut, Jet_eta_cut, Jet_phi_cut, Jet_mass_cut)\",\n",
    "    )\n",
    "\n",
    "    # Build trijet combinations\n",
    "    df = df.Define(\"Trijet_idx\", \"Combinations(Jet_pt_cut, 3)\")\n",
    "\n",
    "    # Trijet_btag is a helpful array mask indicating whether or not the maximum btag value in Trijet is larger than the 0.5 threshold\n",
    "    df = df.Define(\n",
    "        \"Trijet_btag\",\n",
    "        \"\"\"\n",
    "            auto J1_btagCSVV2 = Take(Jet_btagCSVV2_cut, Trijet_idx[0]);\n",
    "            auto J2_btagCSVV2 = Take(Jet_btagCSVV2_cut, Trijet_idx[1]);\n",
    "            auto J3_btagCSVV2 = Take(Jet_btagCSVV2_cut, Trijet_idx[2]);\n",
    "            return J1_btagCSVV2 > 0.5 || J2_btagCSVV2 > 0.5 || J3_btagCSVV2 > 0.5;\n",
    "            \"\"\",\n",
    "    )\n",
    "\n",
    "    # Assign four-momentums to each trijet combination\n",
    "    df = df.Define(\n",
    "        \"Trijet_p4\",\n",
    "        \"\"\"\n",
    "        auto J1 = Take(Jet_p4, Trijet_idx[0]);\n",
    "        auto J2 = Take(Jet_p4, Trijet_idx[1]);\n",
    "        auto J3 = Take(Jet_p4, Trijet_idx[2]);\n",
    "        return (J1+J2+J3)[Trijet_btag];\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    # Get trijet transverse momentum values from four-momentum vectors\n",
    "    df = df.Define(\n",
    "        \"Trijet_pt\",\n",
    "        \"return Map(Trijet_p4, [](const ROOT::Math::PxPyPzMVector &v) { return v.Pt(); })\",\n",
    "    )\n",
    "\n",
    "    # Evaluate mass of trijet with maximum pt and btag higher than threshold\n",
    "    df = df.Define(\"Trijet_mass\", \"Trijet_p4[ArgMax(Trijet_pt)].M()\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def book_histos(\n",
    "    df: ROOT.RDataFrame, process: str, variation: str, nevents: int, inference=False\n",
    ") -> Tuple[list[AGCResult], list[AGCResult]]:\n",
    "    \"\"\"Return the pair of lists of RDataFrame results pertaining to the given process and variation.\n",
    "    The first list contains histograms of reconstructed HT and trijet masses.\n",
    "    The second contains ML inference outputs\"\"\"\n",
    "    # Calculate normalization for MC\n",
    "    x_sec = XSEC_INFO[process]\n",
    "    lumi = 3378  # /pb\n",
    "    xsec_weight = x_sec * lumi / nevents\n",
    "    df = df.Define(\"Weights\", str(xsec_weight))  # default weights\n",
    "\n",
    "    if variation == \"nominal\":\n",
    "        # Jet_pt variations definition\n",
    "        # pt_scale_up() and pt_res_up(jet_pt) return scaling factors applying to jet_pt\n",
    "        # pt_scale_up() - jet energy scaly systematic\n",
    "        # pt_res_up(jet_pt) - jet resolution systematic\n",
    "        df = df.Vary(\n",
    "            \"Jet_pt\",\n",
    "            \"ROOT::RVec<ROOT::RVecF>{Jet_pt*pt_scale_up(), Jet_pt*jet_pt_resolution(Jet_pt)}\",\n",
    "            [\"pt_scale_up\", \"pt_res_up\"],\n",
    "        )\n",
    "\n",
    "        if process == \"wjets\":\n",
    "            # Flat weight variation definition\n",
    "            df = df.Vary(\n",
    "                \"Weights\",\n",
    "                \"Weights*flat_variation()\",\n",
    "                [f\"scale_var_{direction}\" for direction in [\"up\", \"down\"]],\n",
    "            )\n",
    "\n",
    "    # Event selection - the core part of the algorithm applied for both regions\n",
    "    # Selecting events containing at least one lepton and four jets with pT > 30 GeV\n",
    "    # Applying requirement at least one of them must be b-tagged jet (see details in the specification)\n",
    "    df = (\n",
    "        df.Define(\n",
    "            \"Electron_mask\",\n",
    "            \"Electron_pt > 30 && abs(Electron_eta) < 2.1 && Electron_sip3d < 4 && Electron_cutBased == 4\",\n",
    "        )\n",
    "        .Define(\n",
    "            \"Muon_mask\",\n",
    "            \"Muon_pt > 30 && abs(Muon_eta) < 2.1 && Muon_sip3d < 4 && Muon_tightId && Muon_pfRelIso04_all < 0.15\",\n",
    "        )\n",
    "        .Filter(\"Sum(Electron_mask) + Sum(Muon_mask) == 1\")\n",
    "        .Define(\"Jet_mask\", \"Jet_pt > 30 && abs(Jet_eta) < 2.4 && Jet_jetId == 6\")\n",
    "        .Filter(\"Sum(Jet_mask) >= 4\")\n",
    "    )\n",
    "\n",
    "    # create columns for \"good\" jets\n",
    "    df = (\n",
    "        df.Define(\"Jet_pt_cut\", \"Jet_pt[Jet_mask]\")\n",
    "        .Define(\"Jet_btagCSVV2_cut\", \"Jet_btagCSVV2[Jet_mask]\")\n",
    "        .Define(\"Jet_eta_cut\", \"Jet_eta[Jet_mask]\")\n",
    "        .Define(\"Jet_phi_cut\", \"Jet_phi[Jet_mask]\")\n",
    "        .Define(\"Jet_mass_cut\", \"Jet_mass[Jet_mask]\")\n",
    "    )\n",
    "\n",
    "    # b-tagging variations for nominal samples\n",
    "    if variation == \"nominal\":\n",
    "        df = df.Vary(\n",
    "            \"Weights\",\n",
    "            \"ROOT::RVecD{Weights*btag_weight_variation(Jet_pt_cut)}\",\n",
    "            [\n",
    "                f\"{weight_name}_{direction}\"\n",
    "                for weight_name in [f\"btag_var_{i}\" for i in range(4)]\n",
    "                for direction in [\"up\", \"down\"]\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Define HT observable for the 4j1b region\n",
    "    # Only one b-tagged region required\n",
    "    # The observable is the total transvesre momentum\n",
    "    # fmt: off\n",
    "    df4j1b = df.Filter(\"Sum(Jet_btagCSVV2_cut > 0.5) == 1\").Define(\"HT\", \"Sum(Jet_pt_cut)\")\n",
    "    # fmt: on\n",
    "\n",
    "    # Define trijet_mass observable for the 4j2b region (this one is more complicated)\n",
    "    df4j2b = define_trijet_mass(df)\n",
    "\n",
    "    # Book histograms and, if needed, their systematic variations\n",
    "    results = []\n",
    "    for df, observable, region in zip([df4j1b, df4j2b], [\"HT\", \"Trijet_mass\"], [\"4j1b\", \"4j2b\"]):\n",
    "        histo_model = ROOT.RDF.TH1DModel(\n",
    "            name=f\"{region}_{process}_{variation}\",\n",
    "            title=process,\n",
    "            nbinsx=25,\n",
    "            xlow=50,\n",
    "            xup=550,\n",
    "        )\n",
    "        nominal_histo = df.Histo1D(histo_model, observable, \"Weights\")\n",
    "\n",
    "        if variation == \"nominal\":\n",
    "            results.append(\n",
    "                AGCResult(\n",
    "                    nominal_histo,\n",
    "                    region,\n",
    "                    process,\n",
    "                    variation,\n",
    "                    nominal_histo,\n",
    "                    should_vary=True,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            results.append(\n",
    "                AGCResult(\n",
    "                    nominal_histo,\n",
    "                    region,\n",
    "                    process,\n",
    "                    variation,\n",
    "                    nominal_histo,\n",
    "                    should_vary=False,\n",
    "                )\n",
    "            )\n",
    "        print(f\"Booked histogram {histo_model.fName}\")\n",
    "\n",
    "    ml_results: list[AGCResult] = []\n",
    "\n",
    "    if not inference:\n",
    "        return (results, ml_results)\n",
    "\n",
    "    df4j2b = ml.define_features(df4j2b)\n",
    "    df4j2b = ml.infer_output_ml_features(df4j2b)\n",
    "\n",
    "    # Book histograms and, if needed, their systematic variations\n",
    "    for i, feature in enumerate(ml.ml_features_config):\n",
    "        histo_model = ROOT.RDF.TH1DModel(\n",
    "            name=f\"{feature.name}_{process}_{variation}\",\n",
    "            title=feature.title,\n",
    "            nbinsx=feature.binning[0],\n",
    "            xlow=feature.binning[1],\n",
    "            xup=feature.binning[2],\n",
    "        )\n",
    "\n",
    "        nominal_histo = df4j2b.Histo1D(histo_model, f\"results{i}\", \"Weights\")\n",
    "\n",
    "        if variation == \"nominal\":\n",
    "            ml_results.append(\n",
    "                AGCResult(\n",
    "                    nominal_histo,\n",
    "                    feature.name,\n",
    "                    process,\n",
    "                    variation,\n",
    "                    nominal_histo,\n",
    "                    should_vary=True,\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            ml_results.append(\n",
    "                AGCResult(\n",
    "                    nominal_histo,\n",
    "                    feature.name,\n",
    "                    process,\n",
    "                    variation,\n",
    "                    nominal_histo,\n",
    "                    should_vary=False,\n",
    "                )\n",
    "            )\n",
    "        print(f\"Booked histogram {histo_model.fName}\")\n",
    "\n",
    "    # Return the booked results\n",
    "    # Note that no event loop has run yet at this point (RDataFrame is lazy)\n",
    "    return (results, ml_results)\n",
    "\n",
    "\n",
    "def load_cpp():\n",
    "    \"\"\"Load C++ helper functions. Works for both local and distributed execution.\"\"\"\n",
    "    try:\n",
    "        # when using distributed RDataFrame 'helpers.cpp' is copied to the local_directory\n",
    "        # of every worker (via `distribute_unique_paths`)\n",
    "        localdir = get_worker().local_directory\n",
    "        cpp_source = Path(localdir) / \"helpers.h\"\n",
    "    except ValueError:\n",
    "        # must be local execution\n",
    "        cpp_source = \"helpers.h\"\n",
    "\n",
    "    ROOT.gInterpreter.Declare(f'#include \"{str(cpp_source)}\"')\n",
    "\n",
    "\n",
    "def run_mt(\n",
    "    program_start: float,\n",
    "    args: argparse.Namespace,\n",
    "    inputs: list[AGCInput],\n",
    "    results: list[AGCResult],\n",
    "    ml_results: list[AGCResult],\n",
    ") -> None:\n",
    "    ROOT.EnableImplicitMT(args.ncores)\n",
    "    print(f\"Number of threads: {ROOT.GetThreadPoolSize()}\")\n",
    "    load_cpp()\n",
    "    if args.inference:\n",
    "        ml.load_cpp()\n",
    "\n",
    "    for input in inputs:\n",
    "        df = ROOT.RDataFrame(\"Events\", input.paths)\n",
    "        hist_list, ml_hist_list = book_histos(\n",
    "            df, input.process, input.variation, input.nevents, inference=args.inference\n",
    "        )\n",
    "        results += hist_list\n",
    "        ml_results += ml_hist_list\n",
    "\n",
    "    for r in results + ml_results:\n",
    "        if r.should_vary:\n",
    "            r.histo = ROOT.RDF.Experimental.VariationsFor(r.histo)\n",
    "\n",
    "    print(f\"Building the computation graphs took {time() - program_start:.2f} seconds\")\n",
    "\n",
    "    # Run the event loops for all processes and variations here\n",
    "    run_graphs_start = time()\n",
    "    ROOT.RDF.RunGraphs([r.nominal_histo for r in results + ml_results])\n",
    "\n",
    "    print(f\"Executing the computation graphs took {time() - run_graphs_start:.2f} seconds\")\n",
    "\n",
    "\n",
    "def run_distributed(\n",
    "    program_start: float,\n",
    "    args: argparse.Namespace,\n",
    "    inputs: list[AGCInput],\n",
    "    results: list[AGCResult],\n",
    "    ml_results: list[AGCResult],\n",
    ") -> None:\n",
    "    if args.inference:\n",
    "\n",
    "        def ml_init():\n",
    "            load_cpp()\n",
    "            ml.load_cpp()\n",
    "\n",
    "        ROOT.RDF.Distributed.initialize(ml_init)\n",
    "    else:\n",
    "        ROOT.RDF.Distributed.initialize(load_cpp)\n",
    "\n",
    "    scheduler_address = args.scheduler_address if args.scheduler_address else \"\"\n",
    "    client = create_dask_client(args.scheduler, args.ncores, args.hosts, scheduler_address) # it was: with ... as client:\n",
    "    for input in inputs:\n",
    "        df = ROOT.RDataFrame(\n",
    "            \"Events\",\n",
    "            input.paths,\n",
    "            executor=client,\n",
    "            npartitions=args.npartitions,\n",
    "        )\n",
    "        df._headnode.backend.distribute_unique_paths(\n",
    "            [\n",
    "                \"helpers.h\",\n",
    "                \"ml_helpers.h\",\n",
    "                \"ml.py\",\n",
    "                \"models/bdt_even.root\",\n",
    "                \"models/bdt_odd.root\",\n",
    "            ]\n",
    "        )\n",
    "        hist_list, ml_hist_list = book_histos(\n",
    "            df, input.process, input.variation, input.nevents, inference=args.inference\n",
    "        )\n",
    "        results += hist_list\n",
    "        ml_results += ml_hist_list\n",
    "\n",
    "    for r in results + ml_results:\n",
    "        if r.should_vary:\n",
    "            r.histo = ROOT.RDF.Distributed.VariationsFor(r.histo)\n",
    "\n",
    "    print(f\"Building the computation graphs took {time() - program_start:.2f} seconds\")\n",
    "\n",
    "    # Run the event loops for all processes and variations here\n",
    "    run_graphs_start = time()\n",
    "    # for ROOT <v6.28\n",
    "    #ROOT.RDF.Distributed.RunGraphs([r.nominal_histo for r in results + ml_results])\n",
    "    # other ROOT versions\n",
    "    ROOT.RDF.Distributed.RunGraphs([r.histo for r in results + ml_results])\n",
    "    for r in results + ml_results:\n",
    "        h = r.histo\n",
    "        if \"ResultMapProxy\" in str(type(h)):\n",
    "            try:\n",
    "                keys = h.GetKeys()\n",
    "                print(f\"RResultMap with variations: {keys}\")\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Still untriggered map: {h}\\n {e}\")\n",
    "        elif \"ResultPtrProxy\" in str(type(h)):\n",
    "            try:\n",
    "                h.GetValue()  # triggers or accesses the histogram\n",
    "                print(\"RResultPtr has value\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error accessing value: {e}\")\n",
    "    \n",
    "    print(f\"Executing the computation graphs took {time() - run_graphs_start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db7b674-73ca-49ed-8315-318dd3febd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booked histogram 4j1b_ttbar_nominal\n",
      "Booked histogram 4j2b_ttbar_nominal\n",
      "Booked histogram 4j1b_ttbar_scaledown\n",
      "Booked histogram 4j2b_ttbar_scaledown\n",
      "Booked histogram 4j1b_ttbar_scaleup\n",
      "Booked histogram 4j2b_ttbar_scaleup\n",
      "Booked histogram 4j1b_ttbar_ME_var\n",
      "Booked histogram 4j2b_ttbar_ME_var\n",
      "Booked histogram 4j1b_ttbar_PS_var\n",
      "Booked histogram 4j2b_ttbar_PS_var\n",
      "Booked histogram 4j1b_single_top_s_chan_nominal\n",
      "Booked histogram 4j2b_single_top_s_chan_nominal\n",
      "Booked histogram 4j1b_single_top_t_chan_nominal\n",
      "Booked histogram 4j2b_single_top_t_chan_nominal\n",
      "Booked histogram 4j1b_single_top_tW_nominal\n",
      "Booked histogram 4j2b_single_top_tW_nominal\n",
      "Booked histogram 4j1b_wjets_nominal\n",
      "Booked histogram 4j2b_wjets_nominal\n",
      "Building the computation graphs took 12.32 seconds\n",
      "Still untriggered map: <DistRDF.Proxy.ResultMapProxy object at 0x7f11bfed1810>\n",
      " \n",
      "A list of names of systematic variations was requested, but the corresponding map of variations is not\n",
      "present. The variation names cannot be retrieved unless the computation graph has properly run and\n",
      "finished. Something may have gone wrong in the distributed execution, or no variation values were\n",
      "explicitly requested. In the future, it will be possible to get the variation names without triggering.\n",
      "\n",
      "Still untriggered map: <DistRDF.Proxy.ResultMapProxy object at 0x7f11bfed0ed0>\n",
      " \n",
      "A list of names of systematic variations was requested, but the corresponding map of variations is not\n",
      "present. The variation names cannot be retrieved unless the computation graph has properly run and\n",
      "finished. Something may have gone wrong in the distributed execution, or no variation values were\n",
      "explicitly requested. In the future, it will be possible to get the variation names without triggering.\n",
      "\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "RResultPtr has value\n",
      "Still untriggered map: <DistRDF.Proxy.ResultMapProxy object at 0x7f11bfed0dd0>\n",
      " \n",
      "A list of names of systematic variations was requested, but the corresponding map of variations is not\n",
      "present. The variation names cannot be retrieved unless the computation graph has properly run and\n",
      "finished. Something may have gone wrong in the distributed execution, or no variation values were\n",
      "explicitly requested. In the future, it will be possible to get the variation names without triggering.\n",
      "\n",
      "Still untriggered map: <DistRDF.Proxy.ResultMapProxy object at 0x7f11bfed0cd0>\n",
      " \n",
      "A list of names of systematic variations was requested, but the corresponding map of variations is not\n",
      "present. The variation names cannot be retrieved unless the computation graph has properly run and\n",
      "finished. Something may have gone wrong in the distributed execution, or no variation values were\n",
      "explicitly requested. In the future, it will be possible to get the variation names without triggering.\n",
      "\n",
      "RResultMap with variations: ['nominal', 'Jet_pt:pt_res_up', 'Jet_pt:pt_scale_up', 'Weights:btag_var_0_down', 'Weights:btag_var_0_up', 'Weights:btag_var_1_down', 'Weights:btag_var_1_up', 'Weights:btag_var_2_down', 'Weights:btag_var_2_up', 'Weights:btag_var_3_down', 'Weights:btag_var_3_up']\n",
      "RResultMap with variations: ['nominal', 'Jet_pt:pt_res_up', 'Jet_pt:pt_scale_up', 'Weights:btag_var_0_down', 'Weights:btag_var_0_up', 'Weights:btag_var_1_down', 'Weights:btag_var_1_up', 'Weights:btag_var_2_down', 'Weights:btag_var_2_up', 'Weights:btag_var_3_down', 'Weights:btag_var_3_up']\n",
      "RResultMap with variations: ['nominal', 'Jet_pt:pt_res_up', 'Jet_pt:pt_scale_up', 'Weights:btag_var_0_down', 'Weights:btag_var_0_up', 'Weights:btag_var_1_down', 'Weights:btag_var_1_up', 'Weights:btag_var_2_down', 'Weights:btag_var_2_up', 'Weights:btag_var_3_down', 'Weights:btag_var_3_up']\n",
      "RResultMap with variations: ['nominal', 'Jet_pt:pt_res_up', 'Jet_pt:pt_scale_up', 'Weights:btag_var_0_down', 'Weights:btag_var_0_up', 'Weights:btag_var_1_down', 'Weights:btag_var_1_up', 'Weights:btag_var_2_down', 'Weights:btag_var_2_up', 'Weights:btag_var_3_down', 'Weights:btag_var_3_up']\n",
      "RResultMap with variations: ['nominal', 'Jet_pt:pt_res_up', 'Jet_pt:pt_scale_up', 'Weights:btag_var_0_down', 'Weights:btag_var_0_up', 'Weights:btag_var_1_down', 'Weights:btag_var_1_up', 'Weights:btag_var_2_down', 'Weights:btag_var_2_up', 'Weights:btag_var_3_down', 'Weights:btag_var_3_up', 'Weights:scale_var_down', 'Weights:scale_var_up']\n",
      "RResultMap with variations: ['nominal', 'Jet_pt:pt_res_up', 'Jet_pt:pt_scale_up', 'Weights:btag_var_0_down', 'Weights:btag_var_0_up', 'Weights:btag_var_1_down', 'Weights:btag_var_1_up', 'Weights:btag_var_2_down', 'Weights:btag_var_2_up', 'Weights:btag_var_3_down', 'Weights:btag_var_3_up', 'Weights:scale_var_down', 'Weights:scale_var_up']\n",
      "Executing the computation graphs took 35.54 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\nA list of names of systematic variations was requested, but the corresponding map of variations is not\npresent. The variation names cannot be retrieved unless the computation graph has properly run and\nfinished. Something may have gone wrong in the distributed execution, or no variation values were\nexplicitly requested. In the future, it will be possible to get the variation names without triggering.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     31\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAn address of a Dask scheduler was provided but the chosen scheduler is \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.scheduler\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. The \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdask-remote\u001b[39m\u001b[33m'\u001b[39m\u001b[33m scheduler must be chosen if an address is provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     32\u001b[39m         )\n\u001b[32m     33\u001b[39m     run_distributed(program_start, args, inputs, results, ml_results)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m results = \u001b[43mpostprocess_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m save_plots(results)\n\u001b[32m     37\u001b[39m save_histos([r.histo \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results], output_fname=args.output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/workspace/persistent-storage/work/workshop2025_demo/RDataFrame/VBS/analysis-grand-challenge/analyses/cms-open-data-ttbar/utils.py:161\u001b[39m, in \u001b[36mpostprocess_results\u001b[39m\u001b[34m(results)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    158\u001b[39m     resmap, \u001b[33m\"\u001b[39m\u001b[33mGetKeys\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m )  \u001b[38;5;66;03m# RResultMap or distrdf equivalent\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# extract each histogram in the map\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m variation \u001b[38;5;129;01min\u001b[39;00m \u001b[43mresmap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGetKeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    162\u001b[39m     h = resmap[variation]\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# strip the varied variable name: it's always 'weights'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/root/DistRDF/Proxy.py:163\u001b[39m, in \u001b[36mResultMapProxy.GetKeys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03mEquivalent of 'GetKeys' of the RResultMap. Unlike its C++ counterpart,\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[33;03mat the moment we cannot retrieve the list of variation names for a\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    157\u001b[39m \u001b[33;03mshould be aligned with the C++ counterpart.\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proxied_node.value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# TODO:\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# The event loop has not been triggered yet. Currently we can't retrieve\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# the list of variation names without starting the distributed computations\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(textwrap.dedent(\n\u001b[32m    164\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    165\u001b[39m \u001b[33;03m        A list of names of systematic variations was requested, but the corresponding map of variations is not\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[33;03m        present. The variation names cannot be retrieved unless the computation graph has properly run and\u001b[39;00m\n\u001b[32m    167\u001b[39m \u001b[33;03m        finished. Something may have gone wrong in the distributed execution, or no variation values were\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m        explicitly requested. In the future, it will be possible to get the variation names without triggering.\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m))\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: \nA list of names of systematic variations was requested, but the corresponding map of variations is not\npresent. The variation names cannot be retrieved unless the computation graph has properly run and\nfinished. Something may have gone wrong in the distributed execution, or no variation values were\nexplicitly requested. In the future, it will be possible to get the variation names without triggering.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 3.687749 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.369365 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.370351 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.369699 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.368076 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.399171 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.399732 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:962 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Just-in-time compilation phase completed in 0.402681 seconds.\n",
      "Info in <[ROOT.RDF] Info /root_src/tree/dataframe/src/RLoopManager.cxx:948 in void ROOT::Detail::RDF::RLoopManager::Jit()>: Nothing to jit and execute.\n",
      "2025-04-30 15:46:03,132 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "program_start = time()\n",
    "\n",
    "#args, unknown = parse_known_args()\n",
    "#args = parse_args()\n",
    "\n",
    "# Do not add histograms to TDirectories automatically: we'll do it ourselves as needed.\n",
    "ROOT.TH1.AddDirectory(False)\n",
    "# Disable interactive graphics: avoids canvases flashing on screen before we save them to file\n",
    "ROOT.gROOT.SetBatch(True)\n",
    "\n",
    "if args.verbose:\n",
    "    # Set higher RDF verbosity for the rest of the program.\n",
    "    # To only change the verbosity in a given scope, use ROOT.Experimental.RLogScopedVerbosity.\n",
    "    ROOT.Detail.RDF.RDFLogChannel().SetVerbosity(ROOT.ELogLevel(6))\n",
    "\n",
    "inputs: list[AGCInput] = retrieve_inputs(\n",
    "    args.n_max_files_per_sample, args.remote_data_prefix, args.data_cache\n",
    ")\n",
    "results: list[AGCResult] = []\n",
    "ml_results: list[AGCResult] = []\n",
    "\n",
    "if args.scheduler == \"mt\":\n",
    "    run_mt(program_start, args, inputs, results, ml_results)\n",
    "else:\n",
    "    if args.scheduler == \"dask-remote\" and not args.scheduler_address:\n",
    "        raise ValueError(\n",
    "            \"'dask-remote' option chosen but no address provided for the scheduler. Provide it with `--scheduler-address`.\"\n",
    "        )\n",
    "    if args.scheduler_address and args.scheduler != \"dask-remote\":\n",
    "        raise ValueError(\n",
    "            f\"An address of a Dask scheduler was provided but the chosen scheduler is '{args.scheduler}'. The 'dask-remote' scheduler must be chosen if an address is provided.\"\n",
    "        )\n",
    "    run_distributed(program_start, args, inputs, results, ml_results)\n",
    "                \n",
    "results = postprocess_results(results)\n",
    "save_plots(results)\n",
    "save_histos([r.histo for r in results], output_fname=args.output)\n",
    "print(f\"Result histograms saved in file {args.output}\")\n",
    "\n",
    "if args.inference:\n",
    "    ml_results = postprocess_results(ml_results)\n",
    "    save_ml_plots(ml_results)\n",
    "    output_fname = args.output.split(\".root\")[0] + \"_ml_inference.root\"\n",
    "    save_histos([r.histo for r in ml_results], output_fname=output_fname)\n",
    "    print(f\"Result histograms from ML inference step saved in file {output_fname}\")\n",
    "\n",
    "if not args.no_fitting:\n",
    "    fit_histograms(filename=args.output)\n",
    "\n",
    "if args.statistical_validation:\n",
    "    fit_histograms(filename=args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaaa16-3027-4146-9482-dbedba811582",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.scheduler == \"dask-remote\":\n",
    "    client.run(transfer_to_tier) \n",
    "    client.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb525fde-6770-432f-a5b4-915bf5c6cca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "root-build-infn-4 + /scratch",
   "language": "python",
   "name": "python-kernel-from-image-_cvmfs_unpacked.cern.ch_ghcr.io_comp-dev-cms-ita_kernel-rootroot-build-infn-4-1-1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
